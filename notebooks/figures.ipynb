{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle as pkl\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "from torchrl.envs.libs import PettingZooWrapper\n",
    "from torchrl.modules import ProbabilisticActor, TruncatedNormal, MultiAgentMLP\n",
    "from tensordict.nn import TensorDictModule\n",
    "from tensordict.nn.distributions import NormalParamExtractor\n",
    "\n",
    "from public_datasets_game.mechanism import (\n",
    "    PrivateFunding,\n",
    "    QuadraticFunding,\n",
    "    AssuranceContract,\n",
    ")\n",
    "from public_datasets_game.rotting_bandits import (\n",
    "    RottingBanditsGame,\n",
    "    SlidingWindowObsWrapper,\n",
    ")\n",
    "\n",
    "device = torch.device(0) if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "num_experiment_repeats = 5\n",
    "minibatch_size = 1000\n",
    "num_mini_batches = 10\n",
    "frames_per_batch = num_mini_batches * minibatch_size\n",
    "scenario_max_steps = 100\n",
    "\n",
    "\n",
    "experiment_settings = [\n",
    "    (\n",
    "        (\"rb\", 3, 1),\n",
    "        [\n",
    "            (\"collaborative\", \"private\"),\n",
    "            (\"individual\", \"private\"),\n",
    "            (\"individual\", \"quadratic\"),\n",
    "            (\"individual\", \"assurance\"),\n",
    "        ],\n",
    "    ),\n",
    "    (\n",
    "        (\"rb\", 5, 2),\n",
    "        [\n",
    "            (\"collaborative\", \"private\"),\n",
    "            (\"individual\", \"private\"),\n",
    "            (\"individual\", \"quadratic\"),\n",
    "            (\"individual\", \"assurance\"),\n",
    "        ],\n",
    "    ),\n",
    "]\n",
    "\n",
    "\n",
    "def create_env(env_num_bandits, env_num_arms, env_mechanism, env_reward_allocation):\n",
    "    def _create(device):\n",
    "        if env_mechanism == \"private\":\n",
    "            mechanism = PrivateFunding()\n",
    "        elif env_mechanism == \"quadratic\":\n",
    "            mechanism = QuadraticFunding()\n",
    "        elif env_mechanism == \"assurance\":\n",
    "            mechanism = AssuranceContract()\n",
    "        env = SlidingWindowObsWrapper(\n",
    "            env=RottingBanditsGame(\n",
    "                num_bandits=env_num_bandits,\n",
    "                num_arms=env_num_arms,\n",
    "                mechanism=mechanism,\n",
    "                max_steps=scenario_max_steps,\n",
    "                cost_per_play=0.2 * env_num_bandits,\n",
    "                infinite_horizon=True,\n",
    "                reward_allocation=env_reward_allocation,\n",
    "                deficit_resolution=\"tax\",\n",
    "                normalise_action_space=False,\n",
    "                randomise_on_reset=True,\n",
    "                return_funds_info=True,\n",
    "            ),\n",
    "            window_sizes=[5, 25, 125],\n",
    "            flatten_obs=True,\n",
    "        )\n",
    "        env = PettingZooWrapper(env, device=device)\n",
    "\n",
    "        return env\n",
    "\n",
    "    return _create(device)\n",
    "\n",
    "\n",
    "def create_policy(ref_env):\n",
    "    policy_module = TensorDictModule(\n",
    "        module=torch.nn.Sequential(\n",
    "            MultiAgentMLP(\n",
    "                n_agent_inputs=ref_env.num_windows * env_num_arms,\n",
    "                n_agent_outputs=ref_env.action_spec.shape[-1] * 2,\n",
    "                n_agents=env_num_bandits,\n",
    "                centralized=False,\n",
    "                share_params=True,\n",
    "                device=device,\n",
    "                depth=2,\n",
    "                num_cells=128,\n",
    "                activation_class=nn.Tanh,\n",
    "            ),\n",
    "            NormalParamExtractor(),\n",
    "        ),\n",
    "        in_keys=(\"agent\", \"observation\"),\n",
    "        out_keys=[(\"agent\", \"loc\"), (\"agent\", \"scale\")],\n",
    "    )\n",
    "    policy = ProbabilisticActor(\n",
    "        module=policy_module,\n",
    "        spec=ref_env.action_spec,\n",
    "        in_keys=[(\"agent\", \"loc\"), (\"agent\", \"scale\")],\n",
    "        distribution_class=TruncatedNormal,\n",
    "        distribution_kwargs={\n",
    "            \"low\": 0.0,\n",
    "            \"high\": ref_env.action_spec.space.high,\n",
    "        },\n",
    "        # default_interaction_type=ExplorationType.RANDOM,\n",
    "        out_keys=ref_env.action_keys,\n",
    "        return_log_prob=True,\n",
    "        log_prob_key=(\"agent\", \"sample_log_prob\"),\n",
    "    )\n",
    "    return policy\n",
    "\n",
    "\n",
    "experiments = {}\n",
    "\n",
    "for (env_name, env_num_bandits, env_num_arms), settings in experiment_settings:\n",
    "    for env_reward_allocation, env_mechanism in settings:\n",
    "        for experiment_repeat in range(num_experiment_repeats):\n",
    "            output_dir = f\"data/rb_{env_num_bandits}_{env_num_arms}_{env_reward_allocation}_{env_mechanism}_{experiment_repeat}\"\n",
    "            if os.path.exists(output_dir):\n",
    "                with open(\n",
    "                    os.path.join(output_dir, \"train_episode_reward_mean_list\"), \"rb\"\n",
    "                ) as fp:\n",
    "                    train_rewards = pkl.load(fp)\n",
    "                with open(\n",
    "                    os.path.join(output_dir, \"eval_episode_reward_mean_list\"), \"rb\"\n",
    "                ) as fp:\n",
    "                    eval_rewards = pkl.load(fp)\n",
    "                policy_state = torch.load(\n",
    "                    os.path.join(output_dir, \"policy\"), map_location=device\n",
    "                )\n",
    "                experiments[\n",
    "                    (\n",
    "                        env_name,\n",
    "                        env_num_bandits,\n",
    "                        env_num_arms,\n",
    "                        env_reward_allocation,\n",
    "                        env_mechanism,\n",
    "                        experiment_repeat,\n",
    "                    )\n",
    "                ] = {\n",
    "                    \"train_rewards\": train_rewards,\n",
    "                    \"eval_rewards\": eval_rewards,\n",
    "                    \"policy_state\": policy_state,\n",
    "                }\n",
    "            else:\n",
    "                print(f\"{output_dir} does not exist.\")\n",
    "\n",
    "# Display the keys of the loaded experiments\n",
    "print(\"Loaded experiments:\", list(experiments.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loaded_experiments = {}\n",
    "# for key, exp in experiments.items():\n",
    "#     (\n",
    "#         env_name,\n",
    "#         env_num_bandits,\n",
    "#         env_num_arms,\n",
    "#         env_reward_allocation,\n",
    "#         env_mechanism,\n",
    "#         repeat,\n",
    "#     ) = key\n",
    "#     # Create the environment for the experiment\n",
    "#     env = create_env(\n",
    "#         env_num_bandits, env_num_arms, env_mechanism, env_reward_allocation\n",
    "#     )\n",
    "#     # Create the policy using the created environment\n",
    "#     policy = create_policy(env)\n",
    "#     # Load the saved policy state\n",
    "#     policy.load_state_dict(exp[\"policy_state\"])\n",
    "#     loaded_experiments[key] = {\"env\": env, \"policy\": policy}\n",
    "#     print(f\"Loaded environment and policy for experiment: {key}\")\n",
    "\n",
    "#     env.reset(seed=0)\n",
    "#     rollout = env.rollout(max_steps=1000, policy=policy, auto_reset=True)\n",
    "\n",
    "#     print(rollout)\n",
    "#     assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(data, window_size=5):\n",
    "    return np.convolve(data, np.ones(window_size) / window_size, mode=\"valid\")\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "for i, ((env_name, env_num_bandits, env_num_arms), settings) in enumerate(\n",
    "    experiment_settings\n",
    "):\n",
    "    for env_reward_allocation, env_mechanism in settings:\n",
    "        setting_train_rewards = []\n",
    "        for repeat in range(num_experiment_repeats):\n",
    "            key = (\n",
    "                env_name,\n",
    "                env_num_bandits,\n",
    "                env_num_arms,\n",
    "                env_reward_allocation,\n",
    "                env_mechanism,\n",
    "                repeat,\n",
    "            )\n",
    "            if key in experiments:\n",
    "                setting_train_rewards.append(experiments[key][\"train_rewards\"])\n",
    "        if setting_train_rewards:\n",
    "            rewards_array = np.array(setting_train_rewards)\n",
    "            mean_rewards = np.mean(rewards_array, axis=0)\n",
    "            std_rewards = np.std(rewards_array, axis=0)\n",
    "\n",
    "            # Smooth the curves using a moving average of window size 5\n",
    "            ma_mean = moving_average(mean_rewards, 5)\n",
    "            ma_std = moving_average(std_rewards, 5)\n",
    "            # Adjust x_values to match the smoothed data (mode='valid' reduces length by 4)\n",
    "            x_smoothed = np.arange(len(ma_mean)) * frames_per_batch + (\n",
    "                frames_per_batch * 2\n",
    "            )\n",
    "            ax = axs[i]\n",
    "            ax.plot(\n",
    "                x_smoothed, ma_mean, label=f\"{env_reward_allocation}-{env_mechanism}\"\n",
    "            )\n",
    "            ax.fill_between(\n",
    "                x_smoothed,\n",
    "                ma_mean - ma_std,\n",
    "                ma_mean + ma_std,\n",
    "                alpha=0.3,\n",
    "            )\n",
    "\n",
    "\n",
    "def million_formatter(x, pos):\n",
    "    if x >= 1e6:\n",
    "        return f\"{x / 1e6:.0f} million\"\n",
    "    return str(int(x))\n",
    "\n",
    "\n",
    "for ax, exp in zip(axs, experiment_settings):\n",
    "    ax.set_xlabel(\"Collected Frames\")\n",
    "    ax.set_ylabel(\"Mean Episode Reward\")\n",
    "    ax.set_title(f\"Rotting Bandits ({exp[0][1]} consumer | {exp[0][2]} producer)\")\n",
    "    ax.xaxis.set_major_formatter(FuncFormatter(million_formatter))\n",
    "\n",
    "# Collect legend handles and labels from all subplots\n",
    "handles, labels = [], []\n",
    "for ax in axs:\n",
    "    h, l = ax.get_legend_handles_labels()\n",
    "    handles += h\n",
    "    labels += l\n",
    "# Remove duplicates while preserving order\n",
    "by_label = dict(zip(labels, handles))\n",
    "fig.legend(by_label.values(), by_label.keys(), loc=\"upper center\", ncol=len(by_label))\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the settings by the first element and create a new plot for each group.\n",
    "unique_groups = sorted(set(s[0] for s in settings))\n",
    "for group in unique_groups:\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    # Select the settings that belong to the current group.\n",
    "    group_settings = [s for s in settings if s[0] == group]\n",
    "    for setting in group_settings:\n",
    "        setting_train_rewards = []\n",
    "        for repeat in range(num_experiment_repeats):\n",
    "            # Construct full key for experiment lookup.\n",
    "            key = (\n",
    "                env_name,\n",
    "                env_num_bandits,\n",
    "                env_num_arms,\n",
    "                setting[0],\n",
    "                setting[1],\n",
    "                repeat,\n",
    "            )\n",
    "            if key in experiments:\n",
    "                setting_train_rewards.append(experiments[key][\"train_rewards\"])\n",
    "        if setting_train_rewards:\n",
    "            rewards_array = np.array(setting_train_rewards)\n",
    "            mean_rewards = np.mean(rewards_array, axis=0)\n",
    "            std_rewards = np.std(rewards_array, axis=0)\n",
    "            # Smooth the curves using a moving average.\n",
    "            ma_mean = moving_average(mean_rewards, 5)\n",
    "            ma_std = moving_average(std_rewards, 5)\n",
    "            x_smoothed = np.arange(len(ma_mean)) * frames_per_batch + (\n",
    "                frames_per_batch * 2\n",
    "            )\n",
    "\n",
    "            ax.plot(x_smoothed, ma_mean, label=f\"{setting[0]}-{setting[1]}\")\n",
    "            ax.fill_between(x_smoothed, ma_mean - ma_std, ma_mean + ma_std, alpha=0.3)\n",
    "\n",
    "    ax.set_xlabel(\"Collected Frames\")\n",
    "    ax.set_ylabel(\"Mean Train Reward\")\n",
    "    ax.set_title(f\"Train Reward Curves for group: {group}\")\n",
    "    ax.legend()\n",
    "    ax.xaxis.set_major_formatter(FuncFormatter(million_formatter))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

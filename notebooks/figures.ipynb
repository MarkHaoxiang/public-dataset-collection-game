{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle as pkl\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "from torchrl.envs.libs import PettingZooWrapper\n",
    "from torchrl.modules import ProbabilisticActor, TruncatedNormal, MultiAgentMLP\n",
    "from tensordict.nn import TensorDictModule\n",
    "from tensordict.nn.distributions import NormalParamExtractor\n",
    "\n",
    "from public_datasets_game.mechanism import (\n",
    "    PrivateFunding,\n",
    "    QuadraticFunding,\n",
    "    AssuranceContract,\n",
    ")\n",
    "from public_datasets_game.rotting_bandits import (\n",
    "    RottingBanditsGame,\n",
    "    SlidingWindowObsWrapper,\n",
    ")\n",
    "\n",
    "device = torch.device(0) if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "num_experiment_repeats = 5\n",
    "minibatch_size = 1000\n",
    "num_mini_batches = 10\n",
    "frames_per_batch = num_mini_batches * minibatch_size\n",
    "scenario_max_steps = 100\n",
    "\n",
    "\n",
    "experiment_settings = [\n",
    "    (\n",
    "        (\"rb\", 3, 1),\n",
    "        [\n",
    "            (\"collaborative\", \"private\"),\n",
    "            (\"individual\", \"private\"),\n",
    "            (\"individual\", \"quadratic\"),\n",
    "            (\"individual\", \"assurance\"),\n",
    "        ],\n",
    "    ),\n",
    "    (\n",
    "        (\"rb\", 5, 2),\n",
    "        [\n",
    "            (\"collaborative\", \"private\"),\n",
    "            (\"individual\", \"private\"),\n",
    "            (\"individual\", \"quadratic\"),\n",
    "            (\"individual\", \"assurance\"),\n",
    "        ],\n",
    "    ),\n",
    "]\n",
    "\n",
    "\n",
    "def create_env(env_num_bandits, env_num_arms, env_mechanism, env_reward_allocation):\n",
    "    def _create(device):\n",
    "        if env_mechanism == \"private\":\n",
    "            mechanism = PrivateFunding()\n",
    "        elif env_mechanism == \"quadratic\":\n",
    "            mechanism = QuadraticFunding()\n",
    "        elif env_mechanism == \"assurance\":\n",
    "            mechanism = AssuranceContract()\n",
    "        env = SlidingWindowObsWrapper(\n",
    "            env=RottingBanditsGame(\n",
    "                num_bandits=env_num_bandits,\n",
    "                num_arms=env_num_arms,\n",
    "                mechanism=mechanism,\n",
    "                max_steps=scenario_max_steps,\n",
    "                cost_per_play=0.2 * env_num_bandits,\n",
    "                infinite_horizon=True,\n",
    "                reward_allocation=env_reward_allocation,\n",
    "                deficit_resolution=\"tax\",\n",
    "                normalise_action_space=False,\n",
    "                randomise_on_reset=True,\n",
    "                return_funds_info=True,\n",
    "            ),\n",
    "            window_sizes=[5, 25, 125],\n",
    "            flatten_obs=True,\n",
    "        )\n",
    "        env = PettingZooWrapper(env, device=device)\n",
    "\n",
    "        return env\n",
    "\n",
    "    return _create(device)\n",
    "\n",
    "\n",
    "def create_policy(ref_env):\n",
    "    policy_module = TensorDictModule(\n",
    "        module=torch.nn.Sequential(\n",
    "            MultiAgentMLP(\n",
    "                n_agent_inputs=ref_env.num_windows * env_num_arms,\n",
    "                n_agent_outputs=ref_env.action_spec.shape[-1] * 2,\n",
    "                n_agents=env_num_bandits,\n",
    "                centralized=False,\n",
    "                share_params=True,\n",
    "                device=device,\n",
    "                depth=2,\n",
    "                num_cells=128,\n",
    "                activation_class=nn.Tanh,\n",
    "            ),\n",
    "            NormalParamExtractor(),\n",
    "        ),\n",
    "        in_keys=(\"agent\", \"observation\"),\n",
    "        out_keys=[(\"agent\", \"loc\"), (\"agent\", \"scale\")],\n",
    "    )\n",
    "    policy = ProbabilisticActor(\n",
    "        module=policy_module,\n",
    "        spec=ref_env.action_spec,\n",
    "        in_keys=[(\"agent\", \"loc\"), (\"agent\", \"scale\")],\n",
    "        distribution_class=TruncatedNormal,\n",
    "        distribution_kwargs={\n",
    "            \"low\": 0.0,\n",
    "            \"high\": ref_env.action_spec.space.high,\n",
    "        },\n",
    "        # default_interaction_type=ExplorationType.RANDOM,\n",
    "        out_keys=ref_env.action_keys,\n",
    "        return_log_prob=True,\n",
    "        log_prob_key=(\"agent\", \"sample_log_prob\"),\n",
    "    )\n",
    "    return policy\n",
    "\n",
    "\n",
    "experiments = {}\n",
    "\n",
    "for (env_name, env_num_bandits, env_num_arms), settings in experiment_settings:\n",
    "    for env_reward_allocation, env_mechanism in settings:\n",
    "        for experiment_repeat in range(num_experiment_repeats):\n",
    "            output_dir = f\"data/rb_{env_num_bandits}_{env_num_arms}_{env_reward_allocation}_{env_mechanism}_{experiment_repeat}\"\n",
    "            if os.path.exists(output_dir):\n",
    "                with open(\n",
    "                    os.path.join(output_dir, \"train_episode_reward_mean_list\"), \"rb\"\n",
    "                ) as fp:\n",
    "                    train_rewards = pkl.load(fp)\n",
    "                with open(\n",
    "                    os.path.join(output_dir, \"eval_episode_reward_mean_list\"), \"rb\"\n",
    "                ) as fp:\n",
    "                    eval_rewards = pkl.load(fp)\n",
    "                policy_state = torch.load(\n",
    "                    os.path.join(output_dir, \"policy\"), map_location=device\n",
    "                )\n",
    "                experiments[\n",
    "                    (\n",
    "                        env_name,\n",
    "                        env_num_bandits,\n",
    "                        env_num_arms,\n",
    "                        env_reward_allocation,\n",
    "                        env_mechanism,\n",
    "                        experiment_repeat,\n",
    "                    )\n",
    "                ] = {\n",
    "                    \"train_rewards\": train_rewards,\n",
    "                    \"eval_rewards\": eval_rewards,\n",
    "                    \"policy_state\": policy_state,\n",
    "                }\n",
    "            else:\n",
    "                print(f\"{output_dir} does not exist.\")\n",
    "\n",
    "# Display the keys of the loaded experiments\n",
    "print(\"Loaded experiments:\", list(experiments.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"data/funding_amt.pkl\"):\n",
    "    funding_amt = {}\n",
    "    for key, exp in experiments.items():\n",
    "        (\n",
    "            env_name,\n",
    "            env_num_bandits,\n",
    "            env_num_arms,\n",
    "            env_reward_allocation,\n",
    "            env_mechanism,\n",
    "            repeat,\n",
    "        ) = key\n",
    "        # Create the environment for the experiment\n",
    "        env = create_env(\n",
    "            env_num_bandits, env_num_arms, env_mechanism, env_reward_allocation\n",
    "        )\n",
    "        # Create the policy using the created environment\n",
    "        policy = create_policy(env)\n",
    "        # Load the saved policy state\n",
    "        policy.load_state_dict(exp[\"policy_state\"])\n",
    "        print(f\"Loaded environment and policy for experiment: {key}\")\n",
    "\n",
    "        funding = []\n",
    "        for seed in range(100):\n",
    "            env.reset(seed=seed)\n",
    "            td = env.rollout(\n",
    "                max_steps=scenario_max_steps, policy=policy, auto_reset=True\n",
    "            )\n",
    "            info = td.get((\"next\", \"agent\", \"info\"))\n",
    "\n",
    "            funding.append(info[\"funding\"].mean().item())\n",
    "        funding_amt[key] = funding\n",
    "\n",
    "    with open(\"data/funding_amt.pkl\", \"wb\") as fp:\n",
    "        pkl.dump(funding_amt, fp)\n",
    "else:\n",
    "    with open(\"data/funding_amt.pkl\", \"rb\") as fp:\n",
    "        funding_amt = pkl.load(fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(data, window_size=5):\n",
    "    return np.convolve(data, np.ones(window_size) / window_size, mode=\"valid\")\n",
    "\n",
    "\n",
    "fig, all_axs = plt.subplots(2, 2, figsize=(10, 6))\n",
    "\n",
    "fig.suptitle(\"MAPPO Training on Rotting Bandits\")\n",
    "fig.set_constrained_layout(True)\n",
    "\n",
    "axs_train = all_axs[0]  # First row for training rewards\n",
    "axs_funding = all_axs[1]  # Second row for funding box plots\n",
    "\n",
    "# --- TRAINING REWARDS (Top Row) ---\n",
    "for i, ((env_name, env_num_bandits, env_num_arms), settings) in enumerate(\n",
    "    experiment_settings\n",
    "):\n",
    "    for env_reward_allocation, env_mechanism in settings:\n",
    "        setting_train_rewards = []\n",
    "        for repeat in range(num_experiment_repeats):\n",
    "            key = (\n",
    "                env_name,\n",
    "                env_num_bandits,\n",
    "                env_num_arms,\n",
    "                env_reward_allocation,\n",
    "                env_mechanism,\n",
    "                repeat,\n",
    "            )\n",
    "            if key in experiments:\n",
    "                setting_train_rewards.append(experiments[key][\"train_rewards\"])\n",
    "\n",
    "        if setting_train_rewards:\n",
    "            rewards_array = np.array(setting_train_rewards)\n",
    "            mean_rewards = np.mean(rewards_array, axis=0)\n",
    "            std_rewards = np.std(rewards_array, axis=0)\n",
    "\n",
    "            # Smooth the curves using a moving average\n",
    "            ma_mean = moving_average(mean_rewards, 5)\n",
    "            ma_std = moving_average(std_rewards, 5)\n",
    "            x_smoothed = np.arange(len(ma_mean)) * frames_per_batch + (\n",
    "                frames_per_batch * 2\n",
    "            )\n",
    "\n",
    "            ax = axs_train[i]\n",
    "            ax.plot(\n",
    "                x_smoothed, ma_mean, label=f\"{env_reward_allocation}-{env_mechanism}\"\n",
    "            )\n",
    "            ax.fill_between(x_smoothed, ma_mean - ma_std, ma_mean + ma_std, alpha=0.3)\n",
    "\n",
    "\n",
    "def million_formatter(x, pos):\n",
    "    if x >= 1e6:\n",
    "        return f\"{x / 1e6:.0f} million\"\n",
    "    return str(int(x))\n",
    "\n",
    "\n",
    "for ax, exp in zip(axs_train, experiment_settings):\n",
    "    ax.set_xlabel(\"Collected Frames\")\n",
    "    ax.set_ylabel(\"Mean Episode Reward\")\n",
    "    ax.set_title(f\"Training Curve ({exp[0][1]} consumer | {exp[0][2]} producer)\")\n",
    "    ax.xaxis.set_major_formatter(FuncFormatter(million_formatter))\n",
    "\n",
    "# --- FUNDING BOX PLOTS (Bottom Row) ---\n",
    "\n",
    "color_map = list(mcolors.TABLEAU_COLORS.values())\n",
    "\n",
    "for i, ((env_name, env_num_bandits, env_num_arms), settings) in enumerate(\n",
    "    experiment_settings\n",
    "):\n",
    "    funding_data = []\n",
    "    colors = []\n",
    "\n",
    "    for j, (env_reward_allocation, env_mechanism) in enumerate(settings):\n",
    "        color = color_map[j]\n",
    "\n",
    "        for repeat in range(num_experiment_repeats):\n",
    "            key = (\n",
    "                env_name,\n",
    "                env_num_bandits,\n",
    "                env_num_arms,\n",
    "                env_reward_allocation,\n",
    "                env_mechanism,\n",
    "                repeat,\n",
    "            )\n",
    "            if key in funding_amt:\n",
    "                funding_data.append(funding_amt[key])\n",
    "                colors.append(color)\n",
    "\n",
    "    if funding_data:\n",
    "        box = axs_funding[i].boxplot(funding_data, vert=True, patch_artist=True)\n",
    "        for patch, c in zip(box[\"boxes\"], colors):\n",
    "            patch.set_facecolor(c)\n",
    "        axs_funding[i].set_ylabel(\"Funding Distribution\")\n",
    "        axs_funding[i].set_title(\n",
    "            f\"Funding Distributions ({env_num_bandits} consumer | {env_num_arms} producer)\"\n",
    "        )\n",
    "\n",
    "\n",
    "legend_patches = [\n",
    "    mpatches.Patch(\n",
    "        color=color_map[j], label=f\"{env_reward_allocation}, {env_mechanism}\"\n",
    "    )\n",
    "    for j, (env_reward_allocation, env_mechanism) in enumerate(settings)\n",
    "]\n",
    "\n",
    "\n",
    "fig.legend(\n",
    "    handles=legend_patches,\n",
    "    title=\"Reward Allocation & Mechanism\",\n",
    "    loc=\"lower center\",\n",
    "    bbox_to_anchor=(0.5, -0.1),  # Centers below the subplots\n",
    "    ncol=len(legend_patches),  # Spread legend items in a single row\n",
    ")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Save the figure\n",
    "fig.savefig(\"data/basic_training.svg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_settings = [\n",
    "    (\n",
    "        (\"rb\", 3, 1),\n",
    "        [\n",
    "            (\"collaborative\", \"private\"),\n",
    "            (\"individual\", \"private\"),\n",
    "            (\"individual\", \"quadratic\"),\n",
    "            (\"individual\", \"assurance\"),\n",
    "        ],\n",
    "    ),\n",
    "    (\n",
    "        (\"rb\", 9, 1),\n",
    "        [\n",
    "            (\"collaborative\", \"private\"),\n",
    "            (\"individual\", \"private\"),\n",
    "            (\"individual\", \"quadratic\"),\n",
    "            (\"individual\", \"assurance\"),\n",
    "        ],\n",
    "    ),\n",
    "    (\n",
    "        (\"rb\", 27, 1),\n",
    "        [\n",
    "            (\"collaborative\", \"private\"),\n",
    "            (\"individual\", \"private\"),\n",
    "            (\"individual\", \"quadratic\"),\n",
    "            (\"individual\", \"assurance\"),\n",
    "        ],\n",
    "    ),\n",
    "]\n",
    "\n",
    "experiments = {}\n",
    "\n",
    "for (env_name, env_num_bandits, env_num_arms), settings in experiment_settings:\n",
    "    for env_reward_allocation, env_mechanism in settings:\n",
    "        for experiment_repeat in range(num_experiment_repeats):\n",
    "            output_dir = f\"data/rb_{env_num_bandits}_{env_num_arms}_{env_reward_allocation}_{env_mechanism}_{experiment_repeat}\"\n",
    "            if os.path.exists(output_dir):\n",
    "                with open(\n",
    "                    os.path.join(output_dir, \"train_episode_reward_mean_list\"), \"rb\"\n",
    "                ) as fp:\n",
    "                    train_rewards = pkl.load(fp)\n",
    "                with open(\n",
    "                    os.path.join(output_dir, \"eval_episode_reward_mean_list\"), \"rb\"\n",
    "                ) as fp:\n",
    "                    eval_rewards = pkl.load(fp)\n",
    "                policy_state = torch.load(\n",
    "                    os.path.join(output_dir, \"policy\"), map_location=device\n",
    "                )\n",
    "                experiments[\n",
    "                    (\n",
    "                        env_name,\n",
    "                        env_num_bandits,\n",
    "                        env_num_arms,\n",
    "                        env_reward_allocation,\n",
    "                        env_mechanism,\n",
    "                        experiment_repeat,\n",
    "                    )\n",
    "                ] = {\n",
    "                    \"train_rewards\": train_rewards,\n",
    "                    \"eval_rewards\": eval_rewards,\n",
    "                    \"policy_state\": policy_state,\n",
    "                }\n",
    "            else:\n",
    "                print(f\"{output_dir} does not exist.\")\n",
    "\n",
    "# Extract final rewards, averaging over last 5 iterations\n",
    "final_rewards = {}\n",
    "\n",
    "for key, data in experiments.items():\n",
    "    env_name, env_num_bandits, env_num_arms, env_reward_allocation, env_mechanism, _ = (\n",
    "        key\n",
    "    )\n",
    "    last_5_avg = np.mean(data[\"train_rewards\"][-5:])  # Average of last 5 iterations\n",
    "\n",
    "    if env_num_bandits not in final_rewards:\n",
    "        final_rewards[env_num_bandits] = {}\n",
    "\n",
    "    label_order = [\n",
    "        \"collaborative-private\",\n",
    "        \"individual-private\",\n",
    "        \"individual-quadratic\",\n",
    "        \"individual-assurance\",\n",
    "    ]\n",
    "    label = f\"{env_reward_allocation}-{env_mechanism}\"\n",
    "\n",
    "    if label not in final_rewards[env_num_bandits]:\n",
    "        final_rewards[env_num_bandits][label] = []\n",
    "\n",
    "    final_rewards[env_num_bandits][label].append(last_5_avg)\n",
    "\n",
    "# Compute mean and std across repeats, skipping std for 27 bandits\n",
    "bar_data = {}\n",
    "for num_bandits, settings in final_rewards.items():\n",
    "    bar_data[num_bandits] = {}\n",
    "    for label in label_order:\n",
    "        if label in settings:\n",
    "            mean_val = np.mean(settings[label])\n",
    "            std_val = (\n",
    "                np.std(settings[label]) if num_bandits != 27 else 0\n",
    "            )  # Skip std for 27 bandits\n",
    "            bar_data[num_bandits][label] = (mean_val, std_val)\n",
    "\n",
    "\n",
    "# Plotting grouped bar chart\n",
    "num_groups = len(bar_data)\n",
    "bar_labels = label_order\n",
    "num_bars = len(bar_labels)\n",
    "x = np.arange(num_groups)  # Positions for groups\n",
    "bar_width = 0.15\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "colors = plt.cm.tab10.colors  # Use categorical color map\n",
    "\n",
    "for i, label in enumerate(bar_labels):\n",
    "    means = [bar_data[num][label][0] for num in sorted(bar_data.keys())]\n",
    "    stds = [bar_data[num][label][1] for num in sorted(bar_data.keys())]\n",
    "\n",
    "    # Skip std for 27 bandits in error bars\n",
    "    yerr = [\n",
    "        std if num != 27 else np.NaN for std, num in zip(stds, sorted(bar_data.keys()))\n",
    "    ]\n",
    "\n",
    "    ax.bar(\n",
    "        x + i * bar_width,\n",
    "        means,\n",
    "        yerr=yerr,\n",
    "        width=bar_width,\n",
    "        label=label,\n",
    "        color=colors[i % len(colors)],\n",
    "        capsize=5,\n",
    "    )\n",
    "\n",
    "ax.set_xticks(x + (num_bars - 1) * bar_width / 2)\n",
    "ax.set_xticklabels(sorted(bar_data.keys()))\n",
    "ax.set_xlabel(\"Number of Agents\")\n",
    "ax.set_ylabel(\"Trained Episode Reward\")\n",
    "ax.set_yscale(\"log\")\n",
    "ax.set_title(\"Scaling Number of Consumers (1 Producer)\")\n",
    "ax.legend(title=\"Reward Allocation - Mechanism\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "fig.savefig(\"data/scaling.svg\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Literal\n",
    "import pickle as pkl\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tensordict import TensorDict\n",
    "from tensordict.nn import TensorDictModule\n",
    "from tensordict.nn.distributions import NormalParamExtractor\n",
    "from torchrl.objectives import ClipPPOLoss, ValueEstimators\n",
    "from torchrl.envs import TransformedEnv, RewardSum, ParallelEnv\n",
    "from torchrl.envs.libs import PettingZooWrapper\n",
    "from torchrl.envs.utils import check_env_specs, set_exploration_type, ExplorationType\n",
    "from torchrl.modules import ProbabilisticActor, TruncatedNormal, MultiAgentMLP\n",
    "from torchrl.collectors import SyncDataCollector\n",
    "from torchrl.data.replay_buffers import ReplayBuffer\n",
    "from torchrl.data.replay_buffers.samplers import SamplerWithoutReplacement\n",
    "from torchrl.data.replay_buffers.storages import LazyTensorStorage\n",
    "\n",
    "from public_datasets_game.rotting_bandits import (\n",
    "    RottingBanditsGame,\n",
    "    SlidingWindowObsWrapper,\n",
    ")\n",
    "from public_datasets_game.mechanism import (\n",
    "    PrivateFunding,\n",
    "    QuadraticFunding,\n",
    "    AssuranceContract,\n",
    ")\n",
    "\n",
    "\n",
    "device = torch.device(0) if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "scenario_max_steps = 100\n",
    "minibatch_size = 1000\n",
    "num_mini_batches = 10\n",
    "num_iters = 100\n",
    "num_epochs = 4\n",
    "frames_per_batch = num_mini_batches * minibatch_size\n",
    "total_frames = frames_per_batch * num_iters\n",
    "num_train_envs = min(10, frames_per_batch // scenario_max_steps)\n",
    "num_experiment_repeats = 5\n",
    "\n",
    "eval_interval = 10\n",
    "eval_num_iters = 5\n",
    "\n",
    "env_num_bandits = 9\n",
    "env_num_arms = 1\n",
    "\n",
    "\n",
    "def create_env(\n",
    "    env_mechanism, env_reward_allocation, type: Literal[\"ref\", \"train\", \"eval\"] = \"eval\"\n",
    "):\n",
    "    def _create(device):\n",
    "        if env_mechanism == \"private\":\n",
    "            mechanism = PrivateFunding()\n",
    "        elif env_mechanism == \"quadratic\":\n",
    "            mechanism = QuadraticFunding()\n",
    "        elif env_mechanism == \"assurance\":\n",
    "            mechanism = AssuranceContract()\n",
    "\n",
    "        env = SlidingWindowObsWrapper(\n",
    "            env=RottingBanditsGame(\n",
    "                num_bandits=env_num_bandits,\n",
    "                num_arms=env_num_arms,\n",
    "                mechanism=mechanism,\n",
    "                max_steps=scenario_max_steps,\n",
    "                cost_per_play=0.6,\n",
    "                infinite_horizon=True,\n",
    "                reward_allocation=env_reward_allocation,\n",
    "                deficit_resolution=\"tax\",\n",
    "                normalise_action_space=False,\n",
    "                randomise_on_reset=True,\n",
    "                return_funds_info=False,\n",
    "            ),\n",
    "            window_sizes=[5, 25, 125],\n",
    "            flatten_obs=True,\n",
    "        )\n",
    "        env = PettingZooWrapper(env, device=device)\n",
    "\n",
    "        if type == \"train\" or type == \"eval\":\n",
    "            env = TransformedEnv(\n",
    "                env,\n",
    "                RewardSum(\n",
    "                    in_keys=[env.reward_key], out_keys=[(\"agent\", \"episode_reward\")]\n",
    "                ),\n",
    "            )\n",
    "\n",
    "        return env\n",
    "\n",
    "    if type == \"ref\":\n",
    "        return _create(device)\n",
    "    elif type == \"train\":\n",
    "        return ParallelEnv(\n",
    "            num_workers=num_train_envs,\n",
    "            create_env_fn=lambda: _create(\"cpu\"),\n",
    "            device=device,\n",
    "        )\n",
    "    elif type == \"eval\":\n",
    "        return ParallelEnv(\n",
    "            num_workers=eval_num_iters,\n",
    "            create_env_fn=lambda: _create(\"cpu\"),\n",
    "            device=device,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for env_reward_allocation, env_mechanism in [\n",
    "    (\"collaborative\", \"private\"),\n",
    "    (\"individual\", \"private\"),\n",
    "    (\"individual\", \"quadratic\"),\n",
    "    # (\"individual\", \"assurance\"),\n",
    "]:\n",
    "    for experiment_repeat in range(num_experiment_repeats):\n",
    "        output_dir = f\"data/rb_{env_num_bandits}_{env_num_arms}_{env_reward_allocation}_{env_mechanism}_{experiment_repeat}\"\n",
    "\n",
    "        if os.path.exists(output_dir):\n",
    "            print(\"Skipping save: already exists\")\n",
    "            continue\n",
    "\n",
    "        ref_env = create_env(env_mechanism, env_reward_allocation, type=\"ref\")\n",
    "        train_env = create_env(env_mechanism, env_reward_allocation, type=\"train\")\n",
    "        eval_env = create_env(env_mechanism, env_reward_allocation, type=\"eval\")\n",
    "\n",
    "        check_env_specs(ref_env)\n",
    "\n",
    "        policy_module = TensorDictModule(\n",
    "            module=torch.nn.Sequential(\n",
    "                MultiAgentMLP(\n",
    "                    n_agent_inputs=ref_env.num_windows * env_num_arms,\n",
    "                    n_agent_outputs=ref_env.action_spec.shape[-1] * 2,\n",
    "                    n_agents=env_num_bandits,\n",
    "                    centralized=False,\n",
    "                    share_params=True,\n",
    "                    device=device,\n",
    "                    depth=2,\n",
    "                    num_cells=128,\n",
    "                    activation_class=nn.Tanh,\n",
    "                ),\n",
    "                NormalParamExtractor(),\n",
    "            ),\n",
    "            in_keys=(\"agent\", \"observation\"),\n",
    "            out_keys=[(\"agent\", \"loc\"), (\"agent\", \"scale\")],\n",
    "        )\n",
    "        policy = ProbabilisticActor(\n",
    "            module=policy_module,\n",
    "            spec=ref_env.action_spec,\n",
    "            in_keys=[(\"agent\", \"loc\"), (\"agent\", \"scale\")],\n",
    "            distribution_class=TruncatedNormal,\n",
    "            distribution_kwargs={\n",
    "                \"low\": 0.0,\n",
    "                \"high\": ref_env.action_spec.space.high,\n",
    "            },\n",
    "            # default_interaction_type=ExplorationType.RANDOM,\n",
    "            out_keys=ref_env.action_keys,\n",
    "            return_log_prob=True,\n",
    "            log_prob_key=(\"agent\", \"sample_log_prob\"),\n",
    "        )\n",
    "        value = TensorDictModule(\n",
    "            # module=DeepSetsValue(num_windows=ref_env.num_windows),\n",
    "            MultiAgentMLP(\n",
    "                n_agent_inputs=ref_env.num_windows * env_num_arms,\n",
    "                n_agent_outputs=1,\n",
    "                n_agents=env_num_bandits,\n",
    "                centralized=True,\n",
    "                share_params=True,\n",
    "                device=device,\n",
    "                depth=2,\n",
    "                num_cells=128,\n",
    "                activation_class=nn.Tanh,\n",
    "            ),\n",
    "            in_keys=(\"agent\", \"observation\"),\n",
    "            out_keys=[(\"agent\", \"state_value\")],\n",
    "        )\n",
    "\n",
    "        policy.to(device)\n",
    "        value.to(device)\n",
    "\n",
    "        # Check / Initialise\n",
    "        td = ref_env.reset()\n",
    "        with torch.no_grad():\n",
    "            print(policy(td)[(\"agent\", \"action\")].shape)\n",
    "            print(value(td)[(\"agent\", \"state_value\")].shape)\n",
    "\n",
    "        collector = SyncDataCollector(\n",
    "            train_env,\n",
    "            policy,\n",
    "            device=device,\n",
    "            storing_device=device,\n",
    "            frames_per_batch=frames_per_batch,\n",
    "            total_frames=total_frames,\n",
    "            exploration_type=ExplorationType.RANDOM,\n",
    "        )\n",
    "\n",
    "        replay_buffer = ReplayBuffer(\n",
    "            storage=LazyTensorStorage(frames_per_batch, device=device),\n",
    "            sampler=SamplerWithoutReplacement(),\n",
    "            batch_size=minibatch_size,\n",
    "        )\n",
    "\n",
    "        loss_module = ClipPPOLoss(\n",
    "            actor_network=policy, critic_network=value, normalise_advantages=False\n",
    "        )\n",
    "        loss_module.set_keys(\n",
    "            reward=ref_env.reward_key,\n",
    "            action=ref_env.action_key,\n",
    "            sample_log_prob=(\"agent\", \"sample_log_prob\"),\n",
    "            value=(\"agent\", \"state_value\"),\n",
    "            done=(\"agent\", \"done\"),\n",
    "            terminated=(\"agent\", \"terminated\"),\n",
    "        )\n",
    "        loss_module.make_value_estimator(ValueEstimators.GAE, gamma=0.99, lmbda=0.95)\n",
    "        loss_module.to(device=device)\n",
    "        optim = torch.optim.Adam(loss_module.parameters(), 3e-4)\n",
    "\n",
    "        train_episode_reward_mean_list = []\n",
    "        eval_episode_reward_mean_list = []\n",
    "\n",
    "        with tqdm(total=num_iters, desc=\"episode_reward_mean = 0\") as pbar:\n",
    "            for sampling_td in collector:\n",
    "                with torch.no_grad():\n",
    "                    loss_module.value_estimator(\n",
    "                        sampling_td,\n",
    "                        params=loss_module.critic_network_params,\n",
    "                        target_params=loss_module.target_critic_network_params,\n",
    "                    )\n",
    "                data_view = sampling_td.reshape(-1)\n",
    "                replay_buffer.extend(data_view)\n",
    "\n",
    "                for epoch in range(num_epochs):\n",
    "                    for _ in range(frames_per_batch // minibatch_size):\n",
    "                        minibatch: TensorDict = replay_buffer.sample()\n",
    "                        loss_vals = loss_module(minibatch)\n",
    "\n",
    "                        loss_value = (\n",
    "                            loss_vals[\"loss_objective\"]\n",
    "                            + loss_vals[\"loss_critic\"]\n",
    "                            + loss_vals[\"loss_entropy\"]\n",
    "                        )\n",
    "\n",
    "                        loss_value.backward()\n",
    "\n",
    "                        torch.nn.utils.clip_grad_norm_(loss_module.parameters(), 1.0)\n",
    "\n",
    "                        optim.step()\n",
    "                        optim.zero_grad()\n",
    "\n",
    "                    if epoch % eval_interval == 0:\n",
    "                        with set_exploration_type(ExplorationType.DETERMINISTIC):\n",
    "                            reset_td = eval_env.reset(\n",
    "                                list_of_kwargs=[\n",
    "                                    {\"seed\": i} for i in range(eval_num_iters)\n",
    "                                ]\n",
    "                            )\n",
    "                            eval_td = eval_env.rollout(\n",
    "                                max_steps=scenario_max_steps,\n",
    "                                policy=policy,\n",
    "                                auto_cast_to_device=True,\n",
    "                                auto_reset=False,\n",
    "                                tensordict=reset_td,\n",
    "                            )\n",
    "\n",
    "                            done = eval_td.get((\"next\", \"agent\", \"done\"))\n",
    "                            episode_reward_mean = (\n",
    "                                eval_td.get((\"next\", \"agent\", \"episode_reward\"))[done]\n",
    "                                .mean()\n",
    "                                .item()\n",
    "                            )\n",
    "\n",
    "                            eval_episode_reward_mean_list.append(episode_reward_mean)\n",
    "\n",
    "                collector.update_policy_weights_()\n",
    "\n",
    "                # Logging\n",
    "                done = sampling_td.get((\"next\", \"agent\", \"done\"))\n",
    "                episode_reward_mean = (\n",
    "                    sampling_td.get((\"next\", \"agent\", \"episode_reward\"))[done]\n",
    "                    .mean()\n",
    "                    .item()\n",
    "                )\n",
    "                train_episode_reward_mean_list.append(episode_reward_mean)\n",
    "                pbar.set_description(\n",
    "                    f\"train_reward_mean = {episode_reward_mean}, eval_reward_mean = {eval_episode_reward_mean_list[-1]}\",\n",
    "                    refresh=False,\n",
    "                )\n",
    "                pbar.update()\n",
    "\n",
    "        os.makedirs(output_dir)\n",
    "        with open(\n",
    "            os.path.join(output_dir, \"train_episode_reward_mean_list\"), \"wb\"\n",
    "        ) as fp:\n",
    "            pkl.dump(train_episode_reward_mean_list, fp)\n",
    "        with open(\n",
    "            os.path.join(output_dir, \"eval_episode_reward_mean_list\"), \"wb\"\n",
    "        ) as fp:\n",
    "            pkl.dump(eval_episode_reward_mean_list, fp)\n",
    "        torch.save(policy.state_dict(), os.path.join(output_dir, \"policy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

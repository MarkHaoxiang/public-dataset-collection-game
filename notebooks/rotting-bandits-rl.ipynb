{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Literal\n",
    "import pickle as pkl\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tensordict import TensorDict\n",
    "from tensordict.nn import TensorDictModule\n",
    "from tensordict.nn.distributions import NormalParamExtractor\n",
    "from torchrl.objectives import ClipPPOLoss, ValueEstimators\n",
    "from torchrl.envs import TransformedEnv, RewardSum, ParallelEnv\n",
    "from torchrl.envs.libs import PettingZooWrapper\n",
    "from torchrl.envs.utils import check_env_specs, set_exploration_type, ExplorationType\n",
    "from torchrl.modules import ProbabilisticActor, TruncatedNormal, MultiAgentMLP\n",
    "from torchrl.collectors import SyncDataCollector\n",
    "from torchrl.data.replay_buffers import ReplayBuffer\n",
    "from torchrl.data.replay_buffers.samplers import SamplerWithoutReplacement\n",
    "from torchrl.data.replay_buffers.storages import LazyTensorStorage\n",
    "\n",
    "from public_datasets_game.rotting_bandits import (\n",
    "    RottingBanditsGame,\n",
    "    SlidingWindowObsWrapper,\n",
    ")\n",
    "from public_datasets_game.mechanism import (\n",
    "    PrivateFunding,\n",
    "    QuadraticFunding,\n",
    "    AssuranceContract,\n",
    ")\n",
    "\n",
    "\n",
    "device = torch.device(0) if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "scenario_max_steps = 100\n",
    "minibatch_size = 1000\n",
    "num_mini_batches = 10\n",
    "num_iters = 100\n",
    "num_epochs = 4\n",
    "frames_per_batch = num_mini_batches * minibatch_size\n",
    "total_frames = frames_per_batch * num_iters\n",
    "num_train_envs = min(10, frames_per_batch // scenario_max_steps)\n",
    "num_experiment_repeats = 5\n",
    "\n",
    "eval_interval = 10\n",
    "eval_num_iters = 5\n",
    "\n",
    "env_num_bandits = 5\n",
    "env_num_arms = 2\n",
    "\n",
    "\n",
    "def create_env(\n",
    "    env_mechanism, env_reward_allocation, type: Literal[\"ref\", \"train\", \"eval\"] = \"eval\"\n",
    "):\n",
    "    def _create(device):\n",
    "        if env_mechanism == \"private\":\n",
    "            mechanism = PrivateFunding()\n",
    "        elif env_mechanism == \"quadratic\":\n",
    "            mechanism = QuadraticFunding()\n",
    "        elif env_mechanism == \"assurance\":\n",
    "            mechanism = AssuranceContract()\n",
    "\n",
    "        env = SlidingWindowObsWrapper(\n",
    "            env=RottingBanditsGame(\n",
    "                num_bandits=env_num_bandits,\n",
    "                num_arms=env_num_arms,\n",
    "                mechanism=mechanism,\n",
    "                max_steps=scenario_max_steps,\n",
    "                cost_per_play=0.2 * env_num_bandits,\n",
    "                infinite_horizon=True,\n",
    "                reward_allocation=env_reward_allocation,\n",
    "                deficit_resolution=\"tax\",\n",
    "                normalise_action_space=False,\n",
    "                randomise_on_reset=True,\n",
    "                return_funds_info=False,\n",
    "            ),\n",
    "            window_sizes=[5, 25, 125],\n",
    "            flatten_obs=True,\n",
    "        )\n",
    "        env = PettingZooWrapper(env, device=device)\n",
    "\n",
    "        if type == \"train\" or type == \"eval\":\n",
    "            env = TransformedEnv(\n",
    "                env,\n",
    "                RewardSum(\n",
    "                    in_keys=[env.reward_key], out_keys=[(\"agent\", \"episode_reward\")]\n",
    "                ),\n",
    "            )\n",
    "\n",
    "        return env\n",
    "\n",
    "    if type == \"ref\":\n",
    "        return _create(device)\n",
    "    elif type == \"train\":\n",
    "        return ParallelEnv(\n",
    "            num_workers=num_train_envs,\n",
    "            create_env_fn=lambda: _create(\"cpu\"),\n",
    "            device=device,\n",
    "        )\n",
    "    elif type == \"eval\":\n",
    "        return ParallelEnv(\n",
    "            num_workers=eval_num_iters,\n",
    "            create_env_fn=lambda: _create(\"cpu\"),\n",
    "            device=device,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-27 18:41:11,784 [torchrl][INFO] check_env_specs succeeded!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping save: already exists\n",
      "Skipping save: already exists\n",
      "Skipping save: already exists\n",
      "Skipping save: already exists\n",
      "Skipping save: already exists\n",
      "torch.Size([5, 2])\n",
      "torch.Size([5, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/markhaoxiang/Projects/public-datasets-game/.venv/lib/python3.12/site-packages/torchrl/envs/common.py:1103: DeprecationWarning: You are querying a non-trivial, single action_spec, i.e., there is only one action known by the environment but it is not named `'action'`. Currently, env.action_spec returns the leaf but for consistency with the setter, this will return the full spec instead (from v0.8 and on).\n",
      "  warnings.warn(\n",
      "train_reward_mean = 23.07744598388672, eval_reward_mean = 21.169105529785156: 100%|██████████| 100/100 [07:22<00:00,  4.42s/it]\n",
      "2025-03-27 18:48:36,876 [torchrl][INFO] check_env_specs succeeded!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 2])\n",
      "torch.Size([5, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_reward_mean = 22.84939193725586, eval_reward_mean = 21.352426528930664: 100%|██████████| 100/100 [07:11<00:00,  4.32s/it]\n",
      "2025-03-27 18:55:51,206 [torchrl][INFO] check_env_specs succeeded!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 2])\n",
      "torch.Size([5, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_reward_mean = 23.192880630493164, eval_reward_mean = 24.475326538085938: 100%|██████████| 100/100 [07:14<00:00,  4.35s/it]\n",
      "2025-03-27 19:03:08,474 [torchrl][INFO] check_env_specs succeeded!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 2])\n",
      "torch.Size([5, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_reward_mean = 29.006242752075195, eval_reward_mean = 27.042659759521484: 100%|██████████| 100/100 [07:16<00:00,  4.36s/it]\n",
      "2025-03-27 19:10:27,379 [torchrl][INFO] check_env_specs succeeded!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 2])\n",
      "torch.Size([5, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_reward_mean = 23.82802391052246, eval_reward_mean = 21.58485221862793: 100%|██████████| 100/100 [07:08<00:00,  4.29s/it] \n",
      "2025-03-27 19:17:38,536 [torchrl][INFO] check_env_specs succeeded!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 2])\n",
      "torch.Size([5, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_reward_mean = 62.63207244873047, eval_reward_mean = 43.48458480834961: 100%|██████████| 100/100 [07:18<00:00,  4.39s/it] \n",
      "2025-03-27 19:24:59,978 [torchrl][INFO] check_env_specs succeeded!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 2])\n",
      "torch.Size([5, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_reward_mean = 65.30945587158203, eval_reward_mean = 57.25292205810547: 100%|██████████| 100/100 [07:21<00:00,  4.41s/it]  \n",
      "2025-03-27 19:32:24,095 [torchrl][INFO] check_env_specs succeeded!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 2])\n",
      "torch.Size([5, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_reward_mean = 45.41065979003906, eval_reward_mean = 40.28066635131836: 100%|██████████| 100/100 [07:16<00:00,  4.36s/it]  \n",
      "2025-03-27 19:39:42,783 [torchrl][INFO] check_env_specs succeeded!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 2])\n",
      "torch.Size([5, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_reward_mean = 40.087425231933594, eval_reward_mean = 13.71517562866211: 100%|██████████| 100/100 [07:14<00:00,  4.34s/it] \n",
      "2025-03-27 19:46:59,831 [torchrl][INFO] check_env_specs succeeded!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 2])\n",
      "torch.Size([5, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_reward_mean = 54.68444061279297, eval_reward_mean = 44.580291748046875: 100%|██████████| 100/100 [07:16<00:00,  4.37s/it] \n",
      "2025-03-27 19:54:19,406 [torchrl][INFO] check_env_specs succeeded!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 4])\n",
      "torch.Size([5, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_reward_mean = 36.4900016784668, eval_reward_mean = 19.023006439208984: 100%|██████████| 100/100 [07:09<00:00,  4.29s/it] \n",
      "2025-03-27 20:01:31,334 [torchrl][INFO] check_env_specs succeeded!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 4])\n",
      "torch.Size([5, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_reward_mean = 28.033145904541016, eval_reward_mean = 14.043693542480469: 100%|██████████| 100/100 [07:07<00:00,  4.27s/it]\n",
      "2025-03-27 20:08:41,154 [torchrl][INFO] check_env_specs succeeded!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 4])\n",
      "torch.Size([5, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_reward_mean = 41.98060989379883, eval_reward_mean = 19.31824493408203: 100%|██████████| 100/100 [07:04<00:00,  4.25s/it] \n",
      "2025-03-27 20:15:48,475 [torchrl][INFO] check_env_specs succeeded!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 4])\n",
      "torch.Size([5, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_reward_mean = 28.16762924194336, eval_reward_mean = 12.11691951751709: 100%|██████████| 100/100 [07:06<00:00,  4.27s/it] \n",
      "2025-03-27 20:22:57,633 [torchrl][INFO] check_env_specs succeeded!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 4])\n",
      "torch.Size([5, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_reward_mean = 28.905353546142578, eval_reward_mean = 15.888236999511719: 100%|██████████| 100/100 [07:05<00:00,  4.26s/it]\n"
     ]
    }
   ],
   "source": [
    "for env_reward_allocation, env_mechanism in [\n",
    "    (\"collaborative\", \"private\"),\n",
    "    (\"individual\", \"private\"),\n",
    "    (\"individual\", \"quadratic\"),\n",
    "    (\"individual\", \"assurance\"),\n",
    "]:\n",
    "    for experiment_repeat in range(num_experiment_repeats):\n",
    "        output_dir = f\"data/rb_{env_num_bandits}_{env_num_arms}_{env_reward_allocation}_{env_mechanism}_{experiment_repeat}\"\n",
    "\n",
    "        if os.path.exists(output_dir):\n",
    "            print(\"Skipping save: already exists\")\n",
    "            continue\n",
    "\n",
    "        ref_env = create_env(env_mechanism, env_reward_allocation, type=\"ref\")\n",
    "        train_env = create_env(env_mechanism, env_reward_allocation, type=\"train\")\n",
    "        eval_env = create_env(env_mechanism, env_reward_allocation, type=\"eval\")\n",
    "\n",
    "        check_env_specs(ref_env)\n",
    "\n",
    "        policy_module = TensorDictModule(\n",
    "            module=torch.nn.Sequential(\n",
    "                MultiAgentMLP(\n",
    "                    n_agent_inputs=ref_env.num_windows * env_num_arms,\n",
    "                    n_agent_outputs=ref_env.action_spec.shape[-1] * 2,\n",
    "                    n_agents=env_num_bandits,\n",
    "                    centralized=False,\n",
    "                    share_params=True,\n",
    "                    device=device,\n",
    "                    depth=2,\n",
    "                    num_cells=128,\n",
    "                    activation_class=nn.Tanh,\n",
    "                ),\n",
    "                NormalParamExtractor(),\n",
    "            ),\n",
    "            in_keys=(\"agent\", \"observation\"),\n",
    "            out_keys=[(\"agent\", \"loc\"), (\"agent\", \"scale\")],\n",
    "        )\n",
    "        policy = ProbabilisticActor(\n",
    "            module=policy_module,\n",
    "            spec=ref_env.action_spec,\n",
    "            in_keys=[(\"agent\", \"loc\"), (\"agent\", \"scale\")],\n",
    "            distribution_class=TruncatedNormal,\n",
    "            distribution_kwargs={\n",
    "                \"low\": 0.0,\n",
    "                \"high\": ref_env.action_spec.space.high,\n",
    "            },\n",
    "            # default_interaction_type=ExplorationType.RANDOM,\n",
    "            out_keys=ref_env.action_keys,\n",
    "            return_log_prob=True,\n",
    "            log_prob_key=(\"agent\", \"sample_log_prob\"),\n",
    "        )\n",
    "        value = TensorDictModule(\n",
    "            # module=DeepSetsValue(num_windows=ref_env.num_windows),\n",
    "            MultiAgentMLP(\n",
    "                n_agent_inputs=ref_env.num_windows * env_num_arms,\n",
    "                n_agent_outputs=1,\n",
    "                n_agents=env_num_bandits,\n",
    "                centralized=True,\n",
    "                share_params=True,\n",
    "                device=device,\n",
    "                depth=2,\n",
    "                num_cells=128,\n",
    "                activation_class=nn.Tanh,\n",
    "            ),\n",
    "            in_keys=(\"agent\", \"observation\"),\n",
    "            out_keys=[(\"agent\", \"state_value\")],\n",
    "        )\n",
    "\n",
    "        policy.to(device)\n",
    "        value.to(device)\n",
    "\n",
    "        # Check / Initialise\n",
    "        td = ref_env.reset()\n",
    "        with torch.no_grad():\n",
    "            print(policy(td)[(\"agent\", \"action\")].shape)\n",
    "            print(value(td)[(\"agent\", \"state_value\")].shape)\n",
    "\n",
    "        collector = SyncDataCollector(\n",
    "            train_env,\n",
    "            policy,\n",
    "            device=device,\n",
    "            storing_device=device,\n",
    "            frames_per_batch=frames_per_batch,\n",
    "            total_frames=total_frames,\n",
    "            exploration_type=ExplorationType.RANDOM,\n",
    "        )\n",
    "\n",
    "        replay_buffer = ReplayBuffer(\n",
    "            storage=LazyTensorStorage(frames_per_batch, device=device),\n",
    "            sampler=SamplerWithoutReplacement(),\n",
    "            batch_size=minibatch_size,\n",
    "        )\n",
    "\n",
    "        loss_module = ClipPPOLoss(\n",
    "            actor_network=policy, critic_network=value, normalise_advantages=False\n",
    "        )\n",
    "        loss_module.set_keys(\n",
    "            reward=ref_env.reward_key,\n",
    "            action=ref_env.action_key,\n",
    "            sample_log_prob=(\"agent\", \"sample_log_prob\"),\n",
    "            value=(\"agent\", \"state_value\"),\n",
    "            done=(\"agent\", \"done\"),\n",
    "            terminated=(\"agent\", \"terminated\"),\n",
    "        )\n",
    "        loss_module.make_value_estimator(ValueEstimators.GAE, gamma=0.99, lmbda=0.95)\n",
    "        loss_module.to(device=device)\n",
    "        optim = torch.optim.Adam(loss_module.parameters(), 3e-4)\n",
    "\n",
    "        train_episode_reward_mean_list = []\n",
    "        eval_episode_reward_mean_list = []\n",
    "\n",
    "        with tqdm(total=num_iters, desc=\"episode_reward_mean = 0\") as pbar:\n",
    "            for sampling_td in collector:\n",
    "                with torch.no_grad():\n",
    "                    loss_module.value_estimator(\n",
    "                        sampling_td,\n",
    "                        params=loss_module.critic_network_params,\n",
    "                        target_params=loss_module.target_critic_network_params,\n",
    "                    )\n",
    "                data_view = sampling_td.reshape(-1)\n",
    "                replay_buffer.extend(data_view)\n",
    "\n",
    "                for epoch in range(num_epochs):\n",
    "                    for _ in range(frames_per_batch // minibatch_size):\n",
    "                        minibatch: TensorDict = replay_buffer.sample()\n",
    "                        loss_vals = loss_module(minibatch)\n",
    "\n",
    "                        loss_value = (\n",
    "                            loss_vals[\"loss_objective\"]\n",
    "                            + loss_vals[\"loss_critic\"]\n",
    "                            + loss_vals[\"loss_entropy\"]\n",
    "                        )\n",
    "\n",
    "                        loss_value.backward()\n",
    "\n",
    "                        torch.nn.utils.clip_grad_norm_(loss_module.parameters(), 1.0)\n",
    "\n",
    "                        optim.step()\n",
    "                        optim.zero_grad()\n",
    "\n",
    "                    if epoch % eval_interval == 0:\n",
    "                        with set_exploration_type(ExplorationType.DETERMINISTIC):\n",
    "                            reset_td = eval_env.reset(\n",
    "                                list_of_kwargs=[\n",
    "                                    {\"seed\": i} for i in range(eval_num_iters)\n",
    "                                ]\n",
    "                            )\n",
    "                            eval_td = eval_env.rollout(\n",
    "                                max_steps=scenario_max_steps,\n",
    "                                policy=policy,\n",
    "                                auto_cast_to_device=True,\n",
    "                                auto_reset=False,\n",
    "                                tensordict=reset_td,\n",
    "                            )\n",
    "\n",
    "                            done = eval_td.get((\"next\", \"agent\", \"done\"))\n",
    "                            episode_reward_mean = (\n",
    "                                eval_td.get((\"next\", \"agent\", \"episode_reward\"))[done]\n",
    "                                .mean()\n",
    "                                .item()\n",
    "                            )\n",
    "\n",
    "                            eval_episode_reward_mean_list.append(episode_reward_mean)\n",
    "\n",
    "                collector.update_policy_weights_()\n",
    "\n",
    "                # Logging\n",
    "                done = sampling_td.get((\"next\", \"agent\", \"done\"))\n",
    "                episode_reward_mean = (\n",
    "                    sampling_td.get((\"next\", \"agent\", \"episode_reward\"))[done]\n",
    "                    .mean()\n",
    "                    .item()\n",
    "                )\n",
    "                train_episode_reward_mean_list.append(episode_reward_mean)\n",
    "                pbar.set_description(\n",
    "                    f\"train_reward_mean = {episode_reward_mean}, eval_reward_mean = {eval_episode_reward_mean_list[-1]}\",\n",
    "                    refresh=False,\n",
    "                )\n",
    "                pbar.update()\n",
    "\n",
    "        os.makedirs(output_dir)\n",
    "        with open(\n",
    "            os.path.join(output_dir, \"train_episode_reward_mean_list\"), \"wb\"\n",
    "        ) as fp:\n",
    "            pkl.dump(train_episode_reward_mean_list, fp)\n",
    "        with open(\n",
    "            os.path.join(output_dir, \"eval_episode_reward_mean_list\"), \"wb\"\n",
    "        ) as fp:\n",
    "            pkl.dump(eval_episode_reward_mean_list, fp)\n",
    "        torch.save(policy.state_dict(), os.path.join(output_dir, \"policy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

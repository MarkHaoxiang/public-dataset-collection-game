{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tensordict import TensorDict\n",
    "from tensordict.nn import TensorDictModule\n",
    "from tensordict.nn.distributions import NormalParamExtractor\n",
    "from torchrl.objectives import ClipPPOLoss, ValueEstimators\n",
    "from torchrl.envs import TransformedEnv, RewardSum, ParallelEnv\n",
    "from torchrl.envs.libs import PettingZooWrapper\n",
    "from torchrl.envs.utils import check_env_specs, set_exploration_type, ExplorationType\n",
    "from torchrl.modules import ProbabilisticActor, TruncatedNormal\n",
    "from torchrl.collectors import SyncDataCollector\n",
    "from torchrl.data.replay_buffers import ReplayBuffer\n",
    "from torchrl.data.replay_buffers.samplers import SamplerWithoutReplacement\n",
    "from torchrl.data.replay_buffers.storages import LazyTensorStorage\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from public_datasets_game.rotting_bandits import (\n",
    "    RottingBanditsGame,\n",
    "    SlidingWindowObsWrapper,\n",
    ")\n",
    "from public_datasets_game.mechanism import PrivateFunding, QuadraticFundng\n",
    "\n",
    "\n",
    "device = torch.device(0) if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "scenario_max_steps = 100\n",
    "minibatch_size = 1000\n",
    "num_mini_batches = 10\n",
    "num_iters = 100\n",
    "num_epochs = 4\n",
    "frames_per_batch = num_mini_batches * minibatch_size\n",
    "total_frames = frames_per_batch * num_iters\n",
    "num_train_envs = min(10, frames_per_batch // scenario_max_steps)\n",
    "\n",
    "eval_interval = 10\n",
    "eval_num_iters = 5\n",
    "\n",
    "env_num_bandits = 3\n",
    "env_num_arms = 1\n",
    "# env_reward_allocation = \"collaborative\"\n",
    "env_reward_allocation = \"individual\"\n",
    "env_mechanism = \"private\"\n",
    "# env_mechanism = \"quadratic\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mechanism = QuadraticFundng()\n",
    "\n",
    "env = SlidingWindowObsWrapper(\n",
    "    env=RottingBanditsGame(\n",
    "        num_bandits=env_num_bandits,\n",
    "        num_arms=env_num_arms,\n",
    "        mechanism=mechanism,\n",
    "        max_steps=scenario_max_steps,\n",
    "        cost_per_play=0.5,\n",
    "        infinite_horizon=True,\n",
    "        reward_allocation=env_reward_allocation,\n",
    "        deficit_resolution=\"tax\",\n",
    "    ),\n",
    "    window_sizes=[10, 50, 250],\n",
    ")\n",
    "\n",
    "env.reset()\n",
    "\n",
    "env.step({agent: env.action_space(agent).sample() for agent in env.agents})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepSetsLayer(nn.Module):\n",
    "    def __init__(self, emb_dim: int):\n",
    "        super().__init__()\n",
    "        self.transform = nn.Sequential(\n",
    "            nn.Linear(emb_dim, emb_dim), nn.LeakyReLU(), nn.Linear(emb_dim, emb_dim)\n",
    "        )\n",
    "        self.update = nn.Sequential(\n",
    "            nn.Linear(2 * emb_dim, emb_dim), nn.ReLU(), nn.Linear(emb_dim, emb_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.transform(x)\n",
    "        aggr_x = x.sum(dim=-2).unsqueeze(-2).expand(x.shape)\n",
    "        x = self.update(torch.cat([x, aggr_x], dim=-1))\n",
    "        return x\n",
    "\n",
    "\n",
    "class DeepSetsActor(nn.Module):\n",
    "    def __init__(self, num_windows: int, emb_dim: int = 32):\n",
    "        super().__init__()\n",
    "        self.lin_in = nn.Linear(num_windows, emb_dim)\n",
    "        self.deepsets = nn.Sequential(\n",
    "            DeepSetsLayer(emb_dim),\n",
    "            DeepSetsLayer(emb_dim),\n",
    "        )\n",
    "        self.lin_out = nn.Linear(emb_dim, 2)\n",
    "        self.softplus = nn.Softplus()\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.lin_in(x)\n",
    "        x = self.deepsets(x)\n",
    "        x = self.lin_out(x)\n",
    "        x = self.softplus(x)\n",
    "\n",
    "        loc = x[..., 0]\n",
    "        scale = x[..., 1]\n",
    "        x = torch.concat([loc, scale], dim=-1)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class DeepSetsValue(nn.Module):\n",
    "    def __init__(self, num_windows: int, emb_dim: int = 32):\n",
    "        super().__init__()\n",
    "        self.lin_in = nn.Linear(num_windows, emb_dim)\n",
    "        self.deepsets = nn.Sequential(\n",
    "            DeepSetsLayer(emb_dim),\n",
    "            DeepSetsLayer(emb_dim),\n",
    "        )\n",
    "        self.mlp_out = nn.Sequential(\n",
    "            nn.Linear(emb_dim, emb_dim),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(emb_dim, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.lin_in(x)\n",
    "        x = self.deepsets(x)\n",
    "        x = x.mean(dim=-2)\n",
    "        x = self.mlp_out(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def create_env(type: Literal[\"ref\", \"train\", \"eval\"] = \"eval\"):\n",
    "    def _create(device):\n",
    "        if env_mechanism == \"private\":\n",
    "            mechanism = PrivateFunding()\n",
    "        elif env_mechanism == \"quadratic\":\n",
    "            mechanism = QuadraticFundng()\n",
    "\n",
    "        env = SlidingWindowObsWrapper(\n",
    "            env=RottingBanditsGame(\n",
    "                num_bandits=env_num_bandits,\n",
    "                num_arms=env_num_arms,\n",
    "                mechanism=mechanism,\n",
    "                max_steps=scenario_max_steps,\n",
    "                cost_per_play=0.5,\n",
    "                infinite_horizon=True,\n",
    "                reward_allocation=env_reward_allocation,\n",
    "                deficit_resolution=\"tax\",\n",
    "            ),\n",
    "            window_sizes=[10, 50, 250],\n",
    "        )\n",
    "        env = PettingZooWrapper(env, device=device)\n",
    "\n",
    "        if type == \"train\" or type == \"eval\":\n",
    "            env = TransformedEnv(\n",
    "                env,\n",
    "                RewardSum(\n",
    "                    in_keys=[env.reward_key], out_keys=[(\"agent\", \"episode_reward\")]\n",
    "                ),\n",
    "            )\n",
    "\n",
    "        return env\n",
    "\n",
    "    if type == \"ref\":\n",
    "        return _create(device)\n",
    "    elif type == \"train\":\n",
    "        return ParallelEnv(\n",
    "            num_workers=num_train_envs,\n",
    "            create_env_fn=lambda: _create(\"cpu\"),\n",
    "            device=device,\n",
    "        )\n",
    "    elif type == \"eval\":\n",
    "        return ParallelEnv(\n",
    "            num_workers=eval_num_iters,\n",
    "            create_env_fn=lambda: _create(\"cpu\"),\n",
    "            device=device,\n",
    "        )\n",
    "\n",
    "\n",
    "ref_env = create_env(type=\"ref\")\n",
    "train_env = create_env(type=\"train\")\n",
    "eval_env = create_env(type=\"eval\")\n",
    "\n",
    "check_env_specs(ref_env)\n",
    "\n",
    "policy_module = TensorDictModule(\n",
    "    module=torch.nn.Sequential(\n",
    "        DeepSetsActor(num_windows=ref_env.num_windows), NormalParamExtractor()\n",
    "    ),\n",
    "    in_keys=ref_env.observation_keys,\n",
    "    out_keys=[(\"agent\", \"loc\"), (\"agent\", \"scale\")],\n",
    ")\n",
    "policy = ProbabilisticActor(\n",
    "    module=policy_module,\n",
    "    spec=ref_env.action_spec,\n",
    "    in_keys=[(\"agent\", \"loc\"), (\"agent\", \"scale\")],\n",
    "    distribution_class=TruncatedNormal,\n",
    "    distribution_kwargs={\n",
    "        \"low\": 0.0,\n",
    "        \"high\": ref_env._agent_budget_per_collector_step,\n",
    "    },\n",
    "    out_keys=ref_env.action_keys,\n",
    "    return_log_prob=True,\n",
    "    log_prob_key=(\"agent\", \"sample_log_prob\"),\n",
    ")\n",
    "value = TensorDictModule(\n",
    "    module=DeepSetsValue(num_windows=ref_env.num_windows),\n",
    "    in_keys=ref_env.observation_keys,\n",
    "    out_keys=[(\"agent\", \"state_value\")],\n",
    ")\n",
    "\n",
    "policy.to(device)\n",
    "value.to(device)\n",
    "\n",
    "# Check / Initialise\n",
    "td = ref_env.reset()\n",
    "with torch.no_grad():\n",
    "    print(policy(td)[(\"agent\", \"action\")].shape)\n",
    "    print(value(td)[(\"agent\", \"state_value\")].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collector = SyncDataCollector(\n",
    "    train_env,\n",
    "    policy,\n",
    "    device=device,\n",
    "    storing_device=device,\n",
    "    frames_per_batch=frames_per_batch,\n",
    "    total_frames=total_frames,\n",
    "    exploration_type=ExplorationType.RANDOM,\n",
    ")\n",
    "\n",
    "replay_buffer = ReplayBuffer(\n",
    "    storage=LazyTensorStorage(frames_per_batch, device=device),\n",
    "    sampler=SamplerWithoutReplacement(),\n",
    "    batch_size=minibatch_size,\n",
    ")\n",
    "\n",
    "loss_module = ClipPPOLoss(\n",
    "    actor_network=policy, critic_network=value, normalise_advantages=False\n",
    ")\n",
    "loss_module.set_keys(\n",
    "    reward=ref_env.reward_key,\n",
    "    action=ref_env.action_key,\n",
    "    sample_log_prob=(\"agent\", \"sample_log_prob\"),\n",
    "    value=(\"agent\", \"state_value\"),\n",
    "    done=(\"agent\", \"done\"),\n",
    "    terminated=(\"agent\", \"terminated\"),\n",
    ")\n",
    "loss_module.make_value_estimator(ValueEstimators.GAE, gamma=0.99, lmbda=0.95)\n",
    "loss_module.to(device=device)\n",
    "optim = torch.optim.Adam(loss_module.parameters(), 3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_episode_reward_mean_list = []\n",
    "eval_episode_reward_mean_list = []\n",
    "\n",
    "with tqdm(total=num_iters, desc=\"episode_reward_mean = 0\") as pbar:\n",
    "    for sampling_td in collector:\n",
    "        with torch.no_grad():\n",
    "            loss_module.value_estimator(\n",
    "                sampling_td,\n",
    "                params=loss_module.critic_network_params,\n",
    "                target_params=loss_module.target_critic_network_params,\n",
    "            )\n",
    "        data_view = sampling_td.reshape(-1)\n",
    "        replay_buffer.extend(data_view)\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            for _ in range(frames_per_batch // minibatch_size):\n",
    "                minibatch: TensorDict = replay_buffer.sample()\n",
    "                loss_vals = loss_module(minibatch)\n",
    "\n",
    "                loss_value = (\n",
    "                    loss_vals[\"loss_objective\"]\n",
    "                    + loss_vals[\"loss_critic\"]\n",
    "                    + loss_vals[\"loss_entropy\"]\n",
    "                )\n",
    "\n",
    "                loss_value.backward()\n",
    "\n",
    "                torch.nn.utils.clip_grad_norm_(loss_module.parameters(), 1.0)\n",
    "\n",
    "                optim.step()\n",
    "                optim.zero_grad()\n",
    "\n",
    "            if epoch % eval_interval == 0:\n",
    "                with set_exploration_type(ExplorationType.DETERMINISTIC):\n",
    "                    reset_td = eval_env.reset(\n",
    "                        list_of_kwargs=[{\"seed\": i} for i in range(eval_num_iters)]\n",
    "                    )\n",
    "                    eval_td = eval_env.rollout(\n",
    "                        max_steps=scenario_max_steps,\n",
    "                        policy=policy,\n",
    "                        auto_cast_to_device=True,\n",
    "                        auto_reset=False,\n",
    "                        tensordict=reset_td,\n",
    "                    )\n",
    "\n",
    "                    done = eval_td.get((\"next\", \"agent\", \"done\"))\n",
    "                    episode_reward_mean = (\n",
    "                        eval_td.get((\"next\", \"agent\", \"episode_reward\"))[done]\n",
    "                        .mean()\n",
    "                        .item()\n",
    "                    )\n",
    "\n",
    "                    eval_episode_reward_mean_list.append(episode_reward_mean)\n",
    "\n",
    "        collector.update_policy_weights_()\n",
    "\n",
    "        # Logging\n",
    "        done = sampling_td.get((\"next\", \"agent\", \"done\"))\n",
    "        episode_reward_mean = (\n",
    "            sampling_td.get((\"next\", \"agent\", \"episode_reward\"))[done].mean().item()\n",
    "        )\n",
    "        train_episode_reward_mean_list.append(episode_reward_mean)\n",
    "        pbar.set_description(\n",
    "            f\"train_reward_mean = {episode_reward_mean}, eval_reward_mean = {eval_episode_reward_mean_list[-1]}\",\n",
    "            refresh=False,\n",
    "        )\n",
    "        pbar.update()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(eval_episode_reward_mean_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

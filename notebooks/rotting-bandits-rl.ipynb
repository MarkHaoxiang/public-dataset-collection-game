{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tensordict import TensorDict\n",
    "from tensordict.nn import TensorDictModule\n",
    "from tensordict.nn.distributions import NormalParamExtractor\n",
    "from torchrl.objectives import ClipPPOLoss, ValueEstimators\n",
    "from torchrl.envs import TransformedEnv, RewardSum\n",
    "from torchrl.envs.libs import PettingZooWrapper\n",
    "from torchrl.envs.utils import check_env_specs\n",
    "from torchrl.modules import ProbabilisticActor, TruncatedNormal\n",
    "from torchrl.collectors import SyncDataCollector\n",
    "from torchrl.data.replay_buffers import ReplayBuffer\n",
    "from torchrl.data.replay_buffers.samplers import SamplerWithoutReplacement\n",
    "from torchrl.data.replay_buffers.storages import LazyTensorStorage\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from public_datasets_game.rotting_bandits import (\n",
    "    RottingBanditsGame,\n",
    "    SlidingWindowObsWrapper,\n",
    ")\n",
    "from public_datasets_game.mechanism import PrivateFunding\n",
    "\n",
    "\n",
    "device = torch.device(0) if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "minibatch_size = 100\n",
    "n_mini_batches = 10\n",
    "n_iters = 100\n",
    "n_epochs = 4\n",
    "frames_per_batch = n_mini_batches * minibatch_size\n",
    "total_frames = frames_per_batch * n_iters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepSetsLayer(nn.Module):\n",
    "    def __init__(self, emb_dim: int):\n",
    "        \"\"\"DeepSets layer using mean aggregation.\"\"\"\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(emb_dim * 2, emb_dim)\n",
    "        self.sigma = nn.LeakyReLU()\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        mu = x.mean(dim=-2).unsqueeze(-2).expand(x.shape)\n",
    "        x = self.linear(torch.cat([x, mu], dim=-1))\n",
    "        x = self.sigma(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DeepSetsActor(nn.Module):\n",
    "    def __init__(self, num_windows: int, emb_dim: int = 32):\n",
    "        super().__init__()\n",
    "        self.lin_in = nn.Linear(num_windows, emb_dim)\n",
    "        self.deepsets = nn.Sequential(\n",
    "            DeepSetsLayer(emb_dim),\n",
    "            DeepSetsLayer(emb_dim),\n",
    "        )\n",
    "        self.lin_out = nn.Linear(emb_dim, 2)\n",
    "        self.softplus = nn.Softplus()\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.lin_in(x)\n",
    "        x = self.deepsets(x)\n",
    "        x = self.lin_out(x)\n",
    "        x = self.softplus(x)\n",
    "\n",
    "        loc = x[..., 0]\n",
    "        scale = x[..., 1]\n",
    "        x = torch.concat([loc, scale], dim=-1)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class DeepSetsValue(nn.Module):\n",
    "    def __init__(self, num_windows: int, emb_dim: int = 32):\n",
    "        super().__init__()\n",
    "        self.lin_in = nn.Linear(num_windows, emb_dim)\n",
    "        self.deepsets = nn.Sequential(\n",
    "            DeepSetsLayer(emb_dim),\n",
    "            DeepSetsLayer(emb_dim),\n",
    "        )\n",
    "        self.mlp_out = nn.Sequential(\n",
    "            nn.Linear(emb_dim, emb_dim),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(emb_dim, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.lin_in(x)\n",
    "        x = self.deepsets(x)\n",
    "        x = x.mean(dim=-2)\n",
    "        x = self.mlp_out(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "env = SlidingWindowObsWrapper(\n",
    "    env=RottingBanditsGame(\n",
    "        num_bandits=3,\n",
    "        num_arms=5,\n",
    "        mechanism=PrivateFunding(),\n",
    "        max_steps=100,\n",
    "        cost_per_play=0.5,\n",
    "        infinite_horizon=True,\n",
    "    ),\n",
    "    window_sizes=[10, 50, 250],\n",
    ")\n",
    "env = PettingZooWrapper(env, device=device)\n",
    "\n",
    "check_env_specs(env)\n",
    "\n",
    "policy_module = TensorDictModule(\n",
    "    module=torch.nn.Sequential(\n",
    "        DeepSetsActor(num_windows=env.num_windows), NormalParamExtractor()\n",
    "    ),\n",
    "    in_keys=env.observation_keys,\n",
    "    out_keys=[(\"agent\", \"loc\"), (\"agent\", \"scale\")],\n",
    ")\n",
    "policy = ProbabilisticActor(\n",
    "    module=policy_module,\n",
    "    spec=env.action_spec,\n",
    "    in_keys=[(\"agent\", \"loc\"), (\"agent\", \"scale\")],\n",
    "    distribution_class=TruncatedNormal,\n",
    "    distribution_kwargs={\n",
    "        \"low\": 0.0,\n",
    "        \"high\": env._agent_budget_per_collector_step,\n",
    "    },\n",
    "    out_keys=env.action_keys,\n",
    "    return_log_prob=True,\n",
    "    log_prob_key=(\"agent\", \"sample_log_prob\"),\n",
    ")\n",
    "value = TensorDictModule(\n",
    "    module=DeepSetsValue(num_windows=env.num_windows),\n",
    "    in_keys=env.observation_keys,\n",
    "    out_keys=[(\"agent\", \"state_value\")],\n",
    ")\n",
    "\n",
    "policy.to(device)\n",
    "value.to(device)\n",
    "\n",
    "# Check / Initialise\n",
    "td = env.reset()\n",
    "with torch.no_grad():\n",
    "    print(policy(td)[(\"agent\", \"action\")].shape)\n",
    "    print(value(td)[(\"agent\", \"state_value\")].shape)\n",
    "\n",
    "env = TransformedEnv(\n",
    "    env,\n",
    "    RewardSum(in_keys=[env.reward_key], out_keys=[(\"agent\", \"episode_reward\")]),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collector = SyncDataCollector(\n",
    "    env,\n",
    "    policy,\n",
    "    device=device,\n",
    "    storing_device=device,\n",
    "    frames_per_batch=frames_per_batch,\n",
    "    total_frames=total_frames,\n",
    ")\n",
    "\n",
    "replay_buffer = ReplayBuffer(\n",
    "    storage=LazyTensorStorage(frames_per_batch, device=device),\n",
    "    sampler=SamplerWithoutReplacement(),\n",
    "    batch_size=minibatch_size,\n",
    ")\n",
    "\n",
    "loss_module = ClipPPOLoss(\n",
    "    actor_network=policy, critic_network=value, normalise_advantages=False\n",
    ")\n",
    "loss_module.set_keys(\n",
    "    reward=env.reward_key,\n",
    "    action=env.action_key,\n",
    "    sample_log_prob=(\"agent\", \"sample_log_prob\"),\n",
    "    value=(\"agent\", \"state_value\"),\n",
    "    done=(\"agent\", \"done\"),\n",
    "    terminated=(\"agent\", \"terminated\"),\n",
    ")\n",
    "loss_module.make_value_estimator(ValueEstimators.GAE, gamma=0.99, lmbda=0.95)\n",
    "loss_module.to(device=device)\n",
    "optim = torch.optim.Adam(loss_module.parameters(), 3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbar = tqdm(total=n_iters, desc=\"episode_reward_mean = 0\")\n",
    "\n",
    "episode_reward_mean_list = []\n",
    "for sampling_td in collector:\n",
    "    with torch.no_grad():\n",
    "        loss_module.value_estimator(\n",
    "            sampling_td,\n",
    "            params=loss_module.critic_network_params,\n",
    "            target_params=loss_module.target_critic_network_params,\n",
    "        )\n",
    "    data_view = sampling_td.reshape(-1)\n",
    "    replay_buffer.extend(data_view)\n",
    "\n",
    "    for _ in range(n_epochs):\n",
    "        for _ in range(frames_per_batch // minibatch_size):\n",
    "            minibatch: TensorDict = replay_buffer.sample()\n",
    "            loss_vals = loss_module(minibatch)\n",
    "\n",
    "            loss_value = (\n",
    "                loss_vals[\"loss_objective\"]\n",
    "                + loss_vals[\"loss_critic\"]\n",
    "                + loss_vals[\"loss_entropy\"]\n",
    "            )\n",
    "\n",
    "            loss_value.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(loss_module.parameters(), 1.0)\n",
    "\n",
    "            optim.step()\n",
    "            optim.zero_grad()\n",
    "\n",
    "    collector.update_policy_weights_()\n",
    "\n",
    "    # Logging\n",
    "    done = sampling_td.get((\"next\", \"agent\", \"done\"))\n",
    "    episode_reward_mean = (\n",
    "        sampling_td.get((\"next\", \"agent\", \"episode_reward\"))[done].mean().item()\n",
    "    )\n",
    "    episode_reward_mean_list.append(episode_reward_mean)\n",
    "    pbar.set_description(f\"episode_reward_mean = {episode_reward_mean}\", refresh=False)\n",
    "    pbar.update()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(episode_reward_mean_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

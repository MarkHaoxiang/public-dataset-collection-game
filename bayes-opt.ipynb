{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from perlin_numpy import generate_perlin_noise_2d\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "N_AGENTS = 10\n",
    "\n",
    "GRANULARITY = 50\n",
    "\n",
    "y = [\n",
    "    generate_perlin_noise_2d((GRANULARITY, GRANULARITY), (2, 2))\n",
    "    for _ in range(N_AGENTS)\n",
    "]\n",
    "\n",
    "\n",
    "x1 = np.linspace(0, 1 - 0.01, GRANULARITY)\n",
    "x2 = np.linspace(0, 1 - 0.01, GRANULARITY)\n",
    "x1, x2 = np.meshgrid(x1, x2)\n",
    "X = np.stack((x1.flatten(), x2.flatten()), axis=1)\n",
    "Y = y[0].flatten()\n",
    "\n",
    "indices = np.random.choice(X.shape[0], 50, replace=False)\n",
    "X = X[indices]\n",
    "Y = Y[indices, None]\n",
    "\n",
    "# ker = GPy.kern.Matern52(input_dim=2, variance=1.0, lengthscale=1.0)\n",
    "# m = GPy.models.GPRegression(X, Y, ker)\n",
    "\n",
    "# m.optimize(messages=True, max_f_eval=1000)\n",
    "# m.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel\n",
    "from scipy.stats import norm\n",
    "\n",
    "\n",
    "# Define the objective function (replace with your own)\n",
    "def objective_function(x):\n",
    "    return np.sin(5 * x) + np.cos(10 * x) + 0.1 * x**2\n",
    "\n",
    "\n",
    "# Expected Improvement acquisition function\n",
    "def expected_improvement(X, gpr: GaussianProcessRegressor, best_y: float):\n",
    "    mu, sigma = gpr.predict(X, return_std=True)\n",
    "    sigma = np.clip(sigma, 1e-9, None)\n",
    "    improvement = best_y - mu\n",
    "    Z = improvement / sigma\n",
    "    ei = improvement * norm.cdf(Z) + sigma * norm.pdf(Z)\n",
    "    return ei.reshape(-1, 1)\n",
    "\n",
    "\n",
    "# Bayesian Optimization with Active Learning\n",
    "def bayesian_optimization(n_iterations, bounds):\n",
    "    # Initial sample (1 random point within the bounds)\n",
    "    X_samples = np.random.uniform(bounds[0], bounds[1], (1, 1))\n",
    "    y_samples = objective_function(X_samples)\n",
    "\n",
    "    for _ in range(n_iterations):\n",
    "        # Fit Gaussian Process\n",
    "        kernel = ConstantKernel() * RBF(\n",
    "            length_scale=1.0, length_scale_bounds=(1e-1, 1e2)\n",
    "        )\n",
    "        gpr = GaussianProcessRegressor(kernel=kernel)\n",
    "        gpr.fit(X_samples, y_samples)\n",
    "\n",
    "        # Find the best observed value so far\n",
    "        best_y = np.min(y_samples)\n",
    "\n",
    "        # Generate candidate points (grid of 1000 points)\n",
    "        X_candidates = np.linspace(bounds[0], bounds[1], 1000).reshape(-1, 1)\n",
    "\n",
    "        # Compute Expected Improvement (EI)\n",
    "        ei = expected_improvement(X_candidates, gpr, best_y)\n",
    "\n",
    "        plot_progress(X_samples, y_samples, gpr, X_candidates, ei, bounds)\n",
    "\n",
    "        # Select the next point to evaluate (avoid numerical edge cases)\n",
    "        next_x = X_candidates[np.argmax(ei)]\n",
    "        next_y = objective_function(next_x)\n",
    "\n",
    "        # Append new sample (reshape to 2D array)\n",
    "        X_samples = np.vstack([X_samples, next_x.reshape(1, -1)])\n",
    "        y_samples = np.append(y_samples, next_y)\n",
    "\n",
    "    return X_samples, y_samples\n",
    "\n",
    "\n",
    "# Helper function for plotting\n",
    "def plot_progress(X_samples, y_samples, gpr, X_candidates, ei, bounds):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    X_true = np.linspace(bounds[0], bounds[1], 100).reshape(-1, 1)\n",
    "    y_true = objective_function(X_true)\n",
    "\n",
    "    # Plot true function, GPR mean, and uncertainty\n",
    "    plt.plot(X_true, y_true, \"r--\", label=\"True Function\")\n",
    "    mu, sigma = gpr.predict(X_true, return_std=True)\n",
    "    plt.plot(X_true, mu, \"b-\", label=\"GPR Mean\")\n",
    "    plt.fill_between(X_true.ravel(), mu - 1.96 * sigma, mu + 1.96 * sigma, alpha=0.2)\n",
    "\n",
    "    # Plot observed samples\n",
    "    plt.scatter(X_samples, y_samples, c=\"k\", s=100, zorder=10, label=\"Samples\")\n",
    "\n",
    "    # Plot EI on a twin axis\n",
    "    plt.twinx()\n",
    "    plt.plot(X_candidates, ei, \"g--\", alpha=0.5, label=\"Expected Improvement\")\n",
    "    plt.ylabel(\"EI\", color=\"green\")\n",
    "    plt.tick_params(axis=\"y\", labelcolor=\"green\")\n",
    "\n",
    "    plt.title(f\"Iteration {len(X_samples) - 2}: Best y = {np.max(y_samples):.3f}\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Parameters\n",
    "bounds = [0, 5]  # Search space bounds\n",
    "n_iterations = 20  # Number of iterations\n",
    "\n",
    "# Run Bayesian Optimization\n",
    "X_samples, y_samples = bayesian_optimization(n_iterations, bounds)\n",
    "\n",
    "# Print results\n",
    "best_idx = np.argmax(y_samples)\n",
    "print(f\"Optimal x: {X_samples[best_idx][0]:.3f}\")\n",
    "print(f\"Optimal y: {y_samples[best_idx]:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
